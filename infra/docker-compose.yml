version: "3.9"

# =============================================================================
# PRODUCTION-GRADE LOCAL INFRASTRUCTURE
# SQL Server → Kafka (CDC via Debezium) → OpenSearch → FastAPI
# Monitoring: Prometheus + Grafana + Node Exporter + Kafka Exporter
# =============================================================================
# QUICK START:
#   1. cp .env.example .env   (edit passwords)
#   2. sudo sysctl -w vm.max_map_count=262144   (Linux — required for OpenSearch)
#   3. docker compose up --build -d
#   4. docker compose ps      (wait for all services to show "healthy")
# =============================================================================

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "20m"
    max-file: "5"

x-restart: &default-restart
  restart: unless-stopped

networks:
  app-net:
    driver: bridge
    name: app-net
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  sql_data:
  zk_data:
  zk_log:
  kafka_data:
  opensearch_data:
  prometheus_data:
  grafana_data:

services:

  # ===========================================================================
  # SQL SERVER 2022
  # ===========================================================================
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: sqlserver
    <<: *default-restart
    logging: *default-logging
    environment:
      ACCEPT_EULA: "Y"
      MSSQL_SA_PASSWORD: "${SA_PASSWORD}"
      MSSQL_PID: "Developer"
      MSSQL_AGENT_ENABLED: "true"
    ports:
      - "1433:1433"
    volumes:
      - sql_data:/var/opt/mssql
      - ./sql/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test:
        - CMD-SHELL
        - |
          /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa \
            -P "$$MSSQL_SA_PASSWORD" -No -Q "SELECT 1" 2>/dev/null | grep -q "1"
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - app-net

  # ===========================================================================
  # ZOOKEEPER
  # ===========================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    <<: *default-restart
    logging: *default-logging
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
    volumes:
      - zk_data:/var/lib/zookeeper/data
      - zk_log:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep -q imok"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - app-net

  # ===========================================================================
  # KAFKA
  # Internal:  kafka:9092       (container-to-container)
  # External:  localhost:29092  (host machine / IDE / tools)
  # ===========================================================================
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    <<: *default-restart
    logging: *default-logging
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 5368709120
      KAFKA_LOG4J_LOGGERS: "kafka=WARN,org.apache.kafka=WARN"
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list > /dev/null 2>&1"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - app-net

  # ===========================================================================
  # KAFKA INIT — creates all required topics before other services start
  # ===========================================================================
  kafka-init:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Creating Debezium internal topics..."
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists \
          --topic connect-configs  --partitions 1  --replication-factor 1 \
          --config cleanup.policy=compact
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists \
          --topic connect-offsets  --partitions 25 --replication-factor 1 \
          --config cleanup.policy=compact
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists \
          --topic connect-statuses --partitions 5  --replication-factor 1 \
          --config cleanup.policy=compact
        echo "Creating CDC application topics..."
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists \
          --topic cdc.dbo.profiles --partitions 3 --replication-factor 1
        kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists \
          --topic cdc.dbo.users    --partitions 3 --replication-factor 1
        echo "All topics created:"
        kafka-topics --bootstrap-server kafka:9092 --list
    restart: "no"
    networks:
      - app-net

  # ===========================================================================
  # DEBEZIUM / KAFKA CONNECT
  # ===========================================================================
  debezium:
    image: debezium/connect:2.6
    container_name: debezium
    <<: *default-restart
    logging: *default-logging
    depends_on:
      kafka:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: debezium-connect-cluster
      CONFIG_STORAGE_TOPIC: connect-configs
      OFFSET_STORAGE_TOPIC: connect-offsets
      STATUS_STORAGE_TOPIC: connect-statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: All
      KAFKA_OPTS: "-Xms256m -Xmx512m"
      LOG_LEVEL: WARN
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8083/ > /dev/null"]
      interval: 15s
      timeout: 10s
      retries: 8
      start_period: 45s
    networks:
      - app-net

  # ===========================================================================
  # DEBEZIUM INIT — auto-registers SQL Server CDC connector
  # ===========================================================================
  debezium-init:
    image: curlimages/curl:8.6.0
    container_name: debezium-init
    depends_on:
      debezium:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Checking if connector already registered..."
        STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
          http://debezium:8083/connectors/sqlserver-cdc-connector)
        if [ "$$STATUS" = "200" ]; then
          echo "Connector already exists. Skipping."
          exit 0
        fi
        echo "Registering SQL Server CDC connector..."
        curl -sf -X POST http://debezium:8083/connectors \
          -H "Content-Type: application/json" \
          -d "{
            \"name\": \"sqlserver-cdc-connector\",
            \"config\": {
              \"connector.class\": \"io.debezium.connector.sqlserver.SqlServerConnector\",
              \"database.hostname\": \"sqlserver\",
              \"database.port\": \"1433\",
              \"database.user\": \"${CDC_USER}\",
              \"database.password\": \"${CDC_PASSWORD}\",
              \"database.names\": \"${DB_NAME}\",
              \"topic.prefix\": \"cdc\",
              \"table.include.list\": \"${CDC_TABLES}\",
              \"snapshot.mode\": \"initial\",
              \"decimal.handling.mode\": \"string\",
              \"time.precision.mode\": \"connect\",
              \"database.encrypt\": \"false\",
              \"tombstones.on.delete\": \"true\",
              \"heartbeat.interval.ms\": \"30000\",
              \"errors.tolerance\": \"all\",
              \"errors.log.enable\": \"true\",
              \"errors.deadletterqueue.topic.name\": \"cdc.dlq\",
              \"errors.deadletterqueue.topic.replication.factor\": \"1\"
            }
          }" && echo "Connector registered." || echo "WARNING: Registration failed."
        curl -sf http://debezium:8083/connectors/sqlserver-cdc-connector/status
    environment:
      CDC_USER: "${CDC_USER}"
      CDC_PASSWORD: "${CDC_PASSWORD}"
      DB_NAME: "${DB_NAME}"
      CDC_TABLES: "${CDC_TABLES}"
    restart: "no"
    networks:
      - app-net

  # ===========================================================================
  # OPENSEARCH
  # ===========================================================================
  opensearch:
    image: opensearchproject/opensearch:2.14.0
    container_name: opensearch
    <<: *default-restart
    logging: *default-logging
    environment:
      cluster.name: local-cluster
      node.name: opensearch-node-1
      discovery.type: single-node
      plugins.security.disabled: "true"
      OPENSEARCH_JAVA_OPTS: "-Xms${OS_HEAP:-2g} -Xmx${OS_HEAP:-2g}"
      plugins.performance_analyzer.enabled: "true"
      cluster.routing.allocation.disk.watermark.low: "85%"
      cluster.routing.allocation.disk.watermark.high: "90%"
      cluster.routing.allocation.disk.watermark.flood_stage: "95%"
      logger.org.opensearch.action: WARN
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile:  { soft: 65536, hard: 65536 }
    volumes:
      - opensearch_data:/usr/share/opensearch/data
      - ./opensearch/config/opensearch.yml:/usr/share/opensearch/config/opensearch.yml:ro
    ports:
      - "9200:9200"
      - "9600:9600"
    healthcheck:
      test:
        - CMD-SHELL
        - |
          curl -sf http://localhost:9200/_cluster/health | \
            python3 -c "import sys,json; s=json.load(sys.stdin)['status']; sys.exit(0 if s in ['green','yellow'] else 1)"
      interval: 20s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - app-net

  # ===========================================================================
  # OPENSEARCH INIT — creates indexes from mappings/*.json
  # ===========================================================================
  opensearch-init:
    image: curlimages/curl:8.6.0
    container_name: opensearch-init
    depends_on:
      opensearch:
        condition: service_healthy
    volumes:
      - ./opensearch/mappings:/mappings:ro
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Creating OpenSearch indexes..."
        for f in /mappings/*.json; do
          INDEX=$(basename $$f .json)
          EXISTS=$(curl -s -o /dev/null -w "%{http_code}" http://opensearch:9200/$$INDEX)
          if [ "$$EXISTS" = "200" ]; then
            echo "Index $$INDEX already exists, skipping."
          else
            curl -sf -X PUT http://opensearch:9200/$$INDEX \
              -H "Content-Type: application/json" -d @$$f \
              && echo "Created index: $$INDEX" \
              || echo "WARNING: Failed to create $$INDEX"
          fi
        done
        echo "Setting cluster defaults..."
        curl -sf -X PUT http://opensearch:9200/_cluster/settings \
          -H "Content-Type: application/json" \
          -d '{"persistent":{"action.auto_create_index":false}}'
        echo "OpenSearch init complete."
    restart: "no"
    networks:
      - app-net

  # ===========================================================================
  # OPENSEARCH DASHBOARDS
  # ===========================================================================
  dashboards:
    image: opensearchproject/opensearch-dashboards:2.14.0
    container_name: opensearch-dashboards
    <<: *default-restart
    logging: *default-logging
    depends_on:
      opensearch:
        condition: service_healthy
    ports:
      - "5601:5601"
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch:9200"]'
      DISABLE_SECURITY_DASHBOARDS_PLUGIN: "true"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5601/api/status > /dev/null"]
      interval: 20s
      timeout: 10s
      retries: 8
      start_period: 60s
    networks:
      - app-net

  # ===========================================================================
  # INDEXER  (Kafka CDC → OpenSearch)
  # ===========================================================================
  indexer:
    build:
      context: ./indexer
      dockerfile: Dockerfile
    container_name: indexer
    <<: *default-restart
    logging: *default-logging
    depends_on:
      kafka:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      opensearch-init:
        condition: service_completed_successfully
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_GROUP_ID: opensearch-indexer
      KAFKA_TOPICS: "${CDC_TABLES_TOPICS:-cdc.dbo.profiles,cdc.dbo.users}"
      KAFKA_AUTO_OFFSET_RESET: earliest
      OPENSEARCH_HOST: http://opensearch:9200
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    networks:
      - app-net

  # ===========================================================================
  # API  (FastAPI — Search + Metrics)
  # ===========================================================================
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: api
    <<: *default-restart
    logging: *default-logging
    depends_on:
      opensearch:
        condition: service_healthy
    ports:
      - "8000:8000"
    environment:
      OPENSEARCH_HOST: http://opensearch:9200
      DB_HOST: sqlserver
      DB_PORT: 1433
      DB_NAME: "${DB_NAME}"
      DB_USER: "${APP_DB_USER}"
      DB_PASSWORD: "${APP_DB_PASSWORD}"
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
      APP_ENV: "${APP_ENV:-production}"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health > /dev/null"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - app-net

  # ===========================================================================
  # PROMETHEUS
  # ===========================================================================
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus
    <<: *default-restart
    logging: *default-logging
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.enable-lifecycle"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/healthy > /dev/null"]
      interval: 15s
      timeout: 5s
      retries: 5
    networks:
      - app-net

  # ===========================================================================
  # GRAFANA
  # ===========================================================================
  grafana:
    image: grafana/grafana:10.4.2
    container_name: grafana
    <<: *default-restart
    logging: *default-logging
    depends_on:
      prometheus:
        condition: service_healthy
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: "${GRAFANA_USER:-admin}"
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD}"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_INSTALL_PLUGINS: "grafana-piechart-panel"
      GF_ALERTING_ENABLED: "true"
      GF_UNIFIED_ALERTING_ENABLED: "true"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3000/api/health > /dev/null"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - app-net

  # ===========================================================================
  # NODE EXPORTER  (Host OS metrics)
  # ===========================================================================
  node-exporter:
    image: prom/node-exporter:v1.8.1
    container_name: node-exporter
    <<: *default-restart
    logging: *default-logging
    command:
      - "--path.rootfs=/host"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    volumes:
      - /:/host:ro,rslave
    ports:
      - "9100:9100"
    pid: host
    networks:
      - app-net

  # ===========================================================================
  # KAFKA EXPORTER  (Kafka broker + consumer-group metrics)
  # ===========================================================================
  kafka-exporter:
    image: danielqsj/kafka-exporter:v1.7.0
    container_name: kafka-exporter
    <<: *default-restart
    logging: *default-logging
    depends_on:
      kafka:
        condition: service_healthy
    command:
      - "--kafka.server=kafka:9092"
      - "--kafka.version=3.6.0"
      - "--log.level=warn"
    ports:
      - "9308:9308"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9308/metrics > /dev/null"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - app-net
